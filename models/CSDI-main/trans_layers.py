# -*- coding: utf-8 -*-
"""trans_layers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17_hLhEUc6_gauZgTy1dbHRkdEMb0yql5
"""

import math
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
# from torch_geometric.utils import degree
from torch_sparse import SparseTensor, matmul
from torch_geometric.nn import GCNConv,GATConv
import scipy.sparse as sp

#         return out # [B, T_out, N, out_channels]
class TemporalConv1DLayer(nn.Module):
    def __init__(self, Kt, in_channels, out_channels, act_func='relu'):
        """
        Temporal convolution layer for [B, K, L].
        Args:
            Kt: kernel size on L dimension (time/sequence)
            in_channels: K dimension of input
            out_channels: output channels
            act_func: activation function
        """
        super(TemporalConv1DLayer, self).__init__()
        self.Kt = Kt
        self.c_in = in_channels
        self.c_out = out_channels
        self.act_func = act_func

        if act_func == 'GLU':
            out_channels = 2 * out_channels

        self.conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=Kt,
            stride=1,
            padding=(Kt - 1) // 2  # SAME padding
        )
        self.c_out = out_channels

        # Projection if needed
        if in_channels != out_channels:
            self.residual_proj = nn.Conv1d(in_channels, out_channels, kernel_size=1)
        else:
            self.residual_proj = None

    def forward(self, x):
        """
        Args:
            x: Tensor, [B, K, L]
        Returns:
            Tensor, [B, K_out, L_out]
        """
        x_perm = x.permute(0, 2, 1)
        x_conv = self.conv(x_perm)  # [B,  L_out, out_channels,]
        # print(f"x shape: {x.shape}")  # check original x
        # print(f"x_perm shape: {x_perm.shape}")  # check permuted x
        # print(f"x_conv shape: {x_conv.shape}")  # should be [B, C_out, L_out]

        # Residual projection
        if self.residual_proj is not None:
            x_input_proj = self.residual_proj(x_perm)  # [B, K_out, L]
        else:
            x_input_proj = x_perm

        # Conv1d expects [B, C_in, L]
        L_out = x_conv.shape[-1]
        x_input_proj_cropped = x_input_proj[..., :L_out]
                # Activation
        if self.act_func == 'linear':
            out = x_conv
        elif self.act_func == 'sigmoid':
            out = torch.sigmoid(x_conv)
        elif self.act_func == 'relu':
            out = F.relu(x_conv[:, :self.c_out, :] + x_input_proj_cropped)
        elif self.act_func == 'GLU':
            x_proj = x_conv[:, :self.c_out, :]
            x_gate = torch.sigmoid(x_conv[:, self.c_out:, :])
            out = (x_proj + x_input_proj_cropped) * x_gate
        else:
            raise ValueError(f'ERROR: activation function "{self.act_func}" is not defined.')
        out = out.permute(0, 2, 1)
        return out
# import torch

# # Your TemporalConvLayer here (assume already defined)

# # Create layer
# Kt = 3         # Temporal kernel size
# c_in = 16      # Input channels
# c_out = 32     # Output channels
# act_func = 'relu'

# temporal_conv_layer = TemporalConvLayer(Kt=Kt, in_channels=c_in, out_channels=c_out, act_func = act_func)

# # Create dummy input
# # Shape: [B, T, N, c_in]
# B = 2          # batch size
# T = 10         # time steps
# N = 5          # number of nodes

# x = torch.randn(B, T, N, c_in)

# # Apply the layer
# out = temporal_conv_layer(x)

# # Check output shape
# print(f"Input shape : {x.shape}")
# print(f"Output shape: {out.shape}")  # Expect: [B, T_out, N, c_out] where T_out = T - Kt + 1

# graph attention; input: # [B, T_out, N, out_channels] # fixed the T_out, we do the transformer for the graph
# 
class TransConv(nn.Module):
    def __init__(self, in_channels, hidden_channels, adj, num_layers=2, num_heads=1,
                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, use_act=False):
        super().__init__()
        self.adj = adj # sparse matrix
        self.convs = nn.ModuleList()
        self.fcs = nn.ModuleList()
        self.fcs.append(nn.Linear(in_channels, hidden_channels))
        self.bns = nn.ModuleList()
        self.bns.append(nn.LayerNorm(hidden_channels))
        for i in range(num_layers):
            self.convs.append(
                TransConvLayer(hidden_channels, hidden_channels, num_heads=num_heads, use_weight=use_weight))
            self.bns.append(nn.LayerNorm(hidden_channels))

        self.dropout = dropout
        self.activation = F.relu
        self.use_bn = use_bn
        self.residual = use_residual
        self.alpha = alpha
        self.use_act=use_act
        self.edge_index, self.edge_weight = self.get_edge_index_and_weight()

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()
        for bn in self.bns:
            bn.reset_parameters()
        for fc in self.fcs:
            fc.reset_parameters()
#
    def get_edge_index_and_weight(self):
        if isinstance(self.adj, SparseTensor):
            row, col, value = self.adj.coo()
            edge_index = torch.stack([row, col], dim=0)

            if value is None:
                # Use symmetric normalization if no edge weights are present
                deg = torch.bincount(row, minlength=self.adj.size(0)).float()
                deg_inv_sqrt = deg.pow(-0.5)
                deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
                edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]
            else:
                edge_weight = value  # Use original weights

        elif isinstance(self.adj, (np.ndarray, sp.spmatrix)):
            row, col = self.adj.nonzero()
            # row = np.array(row).flatten()
            # col = np.array(col).flatten()
            edge_index = torch.tensor(np.stack([row, col], axis=0), dtype=torch.long)
            deg = np.array(self.adj.sum(axis=1)).flatten()
            deg_inv_sqrt = np.power(deg.astype(np.float64), -0.5) # Use float64 for sqrt stability
            deg_inv_sqrt[np.isinf(deg_inv_sqrt) | np.isnan(deg_inv_sqrt)] = 0 # Handle zeros or NaNs

            # Apply symmetric normalization: edge_weight_ij = deg_i^{-0.5} * deg_j^{-0.5}
            edge_weight = torch.tensor(deg_inv_sqrt[row] * deg_inv_sqrt[col], dtype=torch.float)

        else:
            raise TypeError(f"Unsupported adjacency matrix type: {type(self.adj)}")

        return edge_index, edge_weight



# x means the input feature matrix
# x.shape = [B, T, N, D]

    def forward(self, x):
      """
      x: Tensor of shape [B, T, N, D]
      Output: Tensor of shape [B, T, N, hidden_channels]
      """
    #   print(f"[DEBUG] x.shape = {x.shape}")
      B, K, L = x.shape  # B, K, L = observed_data.shape
      x = x.view(B, K, L)  # [B*T, N, D]

      x = self.fcs[0](x)  # [B*T, N, H]
      if self.use_bn:
          x = self.bns[0](x)
      x = self.activation(x)
      x = F.dropout(x, p=float(self.dropout), training=self.training)

    #   layer_ = [x]
      for i, conv in enumerate(self.convs):
          # input_to_current_conv holds the output of the previous layer (or initial transform)
            input_to_current_conv = x.view(B, K, -1) # Shape: [B*T, N, hidden_channels]

          # TransConvLayer expects [Num_Nodes, Feature_Dim]. Flatten [B*T, N, H] to [B*T*N, H].
        #   input_flat = input_to_current_conv.view(B * K, -1) # Shape: [B*T*N, hidden_channels]
            x_out = []
            attn_out = []
            edge_index = self.edge_index.to(x.device)
            edge_weight = self.edge_weight.to(x.device)
            for b in range(B):
                input_b = input_to_current_conv[b]  # Shape: (K, H)
                x_b, attn_b = conv(input_b, input_b, edge_index=edge_index, edge_weight=edge_weight, output_attn=True)
                x_out.append(x_b)
                attn_out.append(attn_b)

            x = torch.stack(x_out, dim=0)    # (B, K, H)
            attn = torch.stack(attn_out, dim=0)    # (B, K, K)
         # x_flat, attn = conv(x_flat, x_flat, edge_index=None, edge_weight=None, output_attn=True)

            # # Reshape the output back to [B*T, N, hidden_channels] before residual/norm/act
            # x = x_flat.view(B, K, -1) # Shape: [B*T, N, hidden_channels]


          # Apply residual connection
            if self.residual:
                # Ensure dimensions match for residual connection: [B*T, N, H] + [B*T, N, H]
                # x is now the output of the conv layer reshaped: [B*T, N, hidden_channels]
                # input_to_current_conv is the input to this conv layer: [B*T, N, hidden_channels]
                x = self.alpha * x + (1 - self.alpha) * input_to_current_conv


            if self.use_bn:
                # Apply LayerNorm after residual
                # self.bns[i + 1] corresponds to the norm after the i-th conv layer
                x = F.relu(x + input_to_current_conv)
                x = self.bns[i + 1](x)

            if self.use_act:
                x = self.activation(x) # Apply activation after norm

            # Dropout after norm and activation
            x = F.dropout(x, p=float(self.dropout), training=self.training)

            # x is now the processed output of this layer, ready to be the input for the next iteration

                # Reshape back to [B, T, N, hidden_channels] for the final output
            # x = x.view(B, K, -1)
            return x

    def get_attentions(self, x):
        # layer_, attentions = [], []
        attentions = []
        edge_index = self.edge_index.to(x.device)
        edge_weight = self.edge_weight.to(x.device)
        B, K, L = x.shape
        x = x.view(B, K, L)
        x = self.fcs[0](x)
        if self.use_bn:
            x = self.bns[0](x)
        x = self.activation(x)
        # layer_.append(x)
        for i, conv in enumerate(self.convs):
            input_to_conv = x
            x, attn = conv(input_to_conv, input_to_conv, edge_index, edge_weight, output_attn=True)
            attentions.append(attn)
            if self.residual:
                x = self.alpha * x + (1 - self.alpha) * input_to_conv
            if self.use_bn:
                x = self.bns[i + 1](x)
            if self.use_act:
                x = self.activation(x)
            # layer_.append(x)
        # [num_layers, B*T, N, N] — per-head attention maps
        return torch.stack(attentions, dim=0)

# graph attention

class TransConvLayer(nn.Module):
    '''
    Graph-aware Transformer layer with multi-head scaled dot-product attention
    '''

    def __init__(self, in_channels, out_channels, num_heads, use_weight=True):
        super().__init__()
        assert out_channels % num_heads == 0, "out_channels must be divisible by num_heads"

        self.num_heads = num_heads
        self.dim_head = out_channels // num_heads
        self.use_weight = use_weight

        self.Wq = nn.Linear(in_channels, out_channels)
        self.Wk = nn.Linear(in_channels, out_channels)
        self.Wv = nn.Linear(in_channels, out_channels) if use_weight else None
        self.out_proj = nn.Linear(out_channels, out_channels)

        self.dropout = nn.Dropout(0.1)
        self.scale = (self.dim_head) ** -0.5  # Scaling for attention stability
        # add more regularization
        self.norm = nn.LayerNorm(out_channels)
        self.activation = nn.ReLU()

    def reset_parameters(self):
        self.Wq.reset_parameters()
        self.Wk.reset_parameters()
        if self.use_weight:
            self.Wv.reset_parameters()
        self.out_proj.reset_parameters()
        self.norm.reset_parameters()

    def forward(self, query_input, source_input, edge_index=None, edge_weight=None, output_attn=False):
        '''
        query_input, source_input: [N, in_channels]
        Returns:
            final_output: [N, out_channels]
            attn: [H, N, N] (if output_attn)
        '''
        N = query_input.size(0)

        # Linear projections
        Q = self.Wq(query_input)  # [N, H * D]
        K = self.Wk(source_input)
        V = self.Wv(source_input) if self.use_weight else source_input

        # Reshape for multi-head: [N, H, D]
        Q = Q.view(N, self.num_heads, self.dim_head)
        K = K.view(N, self.num_heads, self.dim_head)
        V = V.view(N, self.num_heads, self.dim_head)

        # Scaled dot-product attention
        attn_scores = torch.einsum('nhd,mhd->hnm', Q, K) * self.scale  # [H, N, N]
        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Attention output
        out = torch.einsum('hnm,mhd->nhd', attn_weights, V)  # [N, H, D]
        out = out.reshape(N, -1)  # [N, H*D]
        out = self.out_proj(out)  # [N, out_channels]
        out = self.dropout(self.activation(self.norm(out)))
        if output_attn:
            return out, attn_weights
        else:
            return out


class GAT(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, adj,num_layers=2,
                 dropout=0.5, use_bn=True, heads=2, out_heads=1):
        super(GAT, self).__init__()
        self.adj = adj
        self.convs = nn.ModuleList()
        self.convs.append(
            GATConv(in_channels, hidden_channels, dropout=dropout, heads=heads, concat=True))

        self.bns = nn.ModuleList()
        self.bns.append(nn.BatchNorm1d(hidden_channels*heads))
        for _ in range(num_layers - 2):
            self.convs.append(
                    GATConv(hidden_channels*heads, hidden_channels, dropout=dropout, heads=heads, concat=True))
            self.bns.append(nn.BatchNorm1d(hidden_channels*heads))

        self.convs.append(
            GATConv(hidden_channels*heads, out_channels, dropout=dropout, heads=out_heads, concat=False))

        self.dropout = dropout
        self.activation = F.elu
        self.use_bn = use_bn

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()
        for bn in self.bns:
            bn.reset_parameters()

    def forward(self, data):
        device = data.device
        edge_index=torch.tensor(np.array(self.adj.nonzero()), dtype=torch.long).to(device)

        batch_size, seq_len, in_channels = data.shape
        x = data.reshape(batch_size * seq_len, in_channels)

        x = F.dropout(x, p=self.dropout, training=self.training)
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            if self.use_bn:
                x = self.bns[i](x)
            x = self.activation(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index)
        x = x.view(batch_size, seq_len, -1)
        return x

class GCN(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, adj, num_layers=2,
                 dropout=0.5, save_mem=True, use_bn=True):
        super(GCN, self).__init__()
        self.adj = adj
        self.convs = nn.ModuleList()
        self.convs.append(
            GCNConv(in_channels, hidden_channels, cached=not save_mem))

        self.bns = nn.ModuleList()
        self.bns.append(nn.BatchNorm1d(hidden_channels))
        for _ in range(num_layers - 2):
            self.convs.append(
                GCNConv(hidden_channels, hidden_channels, cached=not save_mem))
            self.bns.append(nn.BatchNorm1d(hidden_channels))
        self.convs.append(
            GCNConv(hidden_channels, out_channels, cached=not save_mem))

        self.dropout = dropout
        self.activation = F.relu
        self.use_bn = use_bn

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()
        for bn in self.bns:
            bn.reset_parameters()

    def forward(self, x):
        device = x.device

        # Use coo() to get edge_index from SparseTensor
        row, col = self.adj.nonzero()
        edge_index = torch.tensor(np.vstack([row, col]), dtype=torch.long).to(device)
        # print(x.shape)
        batch_size, seq_len, in_channels = x.shape
        x = x.reshape(batch_size * seq_len, in_channels)

        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            if self.use_bn:
                x = self.bns[i](x)
            x = self.activation(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.convs[-1](x, edge_index)
        x = x.reshape(batch_size, seq_len, -1)
        return x

def full_attention_conv(qs, ks, vs, output_attn=False):
    # normalize input
    # Avoid division by zero or NaN if norm is zero
    qs_norm = torch.norm(qs, p=2, dim=-1, keepdim=True)
    qs = qs / (qs_norm + 1e-8) # Add small epsilon for stability

    ks_norm = torch.norm(ks, p=2, dim=-1, keepdim=True)
    ks = ks / (ks_norm + 1e-8) # Add small epsilon for stability

    N = qs.shape[0] # Number of query nodes (B*T*N)

    # numerator
    kvs = torch.einsum("lhm,lhd->hmd", ks, vs) # [N_source, H, M] x [N_source, H, D'] -> [H, M, D']
    attention_num = torch.einsum("nhm,hmd->nhd", qs, kvs)  # [N_query, H, M] x [H, M, D'] -> [N_query, H, D']
    attention_num += torch.mean(vs, dim=0, keepdim=True) # Add mean of values to numerator

    # denominator
    all_ones = torch.ones([ks.shape[0]], device=ks.device) # ones tensor with size of N_source
    ks_sum = torch.einsum("lhm,l->hm", ks, all_ones) # [N_source, H, M] x [N_source] -> [H, M]
    attention_normalizer = torch.einsum("nhm,hm->nh", qs, ks_sum)  # [N_query, H, M] x [H, M] -> [N_query, H]

    # attentive aggregated results
    attention_normalizer = torch.unsqueeze(
        attention_normalizer, -1)  # [N_query, H, 1]
    attention_normalizer += torch.ones_like(attention_normalizer) * N # Add N (num query nodes) to denominator
    # Avoid division by zero
    attn_output = attention_num / (attention_normalizer + 1e-6)  # [N_query, H, D']

    # compute attention for visualization if needed
    if output_attn:
        # attention matrix shape [N_query, N_source, H]
        attention=torch.einsum("nhm,lhm->nlh", qs, ks)
        # Average over heads
        attention=attention.mean(dim=-1) #[N_query, N_source]

        # Normalizer for the attention matrix itself
        # Using the same normalizer as the output aggregation, averaged over heads
        normalizer=attention_normalizer.squeeze(dim=-1).mean(dim=-1,keepdims=True) #[N_query, 1]
        # Avoid division by zero
        attention=attention / (normalizer + 1e-8)


    if output_attn:
        return attn_output, attention
    else:
        return attn_output

class GraphTransformer(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, adj, num_layers=2, num_heads=1,
                 alpha=0.5, dropout=0.5, use_bn=True, use_residual=True, use_weight=True, graph_weight=0.8):
        super().__init__()
        self.temp_conv = TemporalConv1DLayer(3, in_channels, out_channels, act_func='relu') # Kt=3; (B, K, T)
        self.trans_conv = TransConv(in_channels, hidden_channels, adj, num_layers, num_heads, alpha, dropout, use_bn, use_residual, use_weight)
        self.gnn = GAT(in_channels, hidden_channels, out_channels, adj, num_layers, dropout)
        self.graph_weight = graph_weight
        self.fc = nn.Linear(hidden_channels, out_channels)

        # self.params1 = list(self.trans_conv.parameters())
        # self.params2 = list(self.gnn.parameters()) if self.gnn is not None else []
        # self.params2.extend(list(self.fc.parameters()))

    def forward(self, x):
        x = self.temp_conv(x)
        x1 = self.trans_conv(x)        # [B, T, hidden]
        x1 = self.fc(x1)               # [B, T, out]
        x1 = F.relu(x1)
        x1 = x1 * 2
        x2 = self.gnn(x)               # [B, T, out]
        print(f"self.graph_weight", self.graph_weight)
        print(f"corresponding x2:", x2)
        print(f"corresponding x1", x1)
        x = self.graph_weight * x2 + (1 - self.graph_weight) * x1
        return x


    def get_attentions(self, x):
        return self.trans_conv.get_attentions(x)

    def reset_parameters(self):
        self.trans_conv.reset_parameters()
        if self.gnn is not None:
            self.gnn.reset_parameters()
        self.fc.reset_parameters()


if __name__ == "__main__":

    # 假设 N 是节点数, adj 是邻接矩阵 (稀疏形式)
    def generate_dummy_adj(N, density=0.1):
        adj = sp.eye(N)
        num_edges = int(N * N * density)
        row = np.random.randint(0, N, size=num_edges)
        col = np.random.randint(0, N, size=num_edges)
        data = np.ones(num_edges)
        random_adj = sp.coo_matrix((data, (row, col)), shape=(N, N))
        adj = (adj + random_adj).tocoo()
        return adj

    B = 2   # batch size
    T = 5   # time steps
    N = 4   # num nodes (你如果 GCN 要用到)
    F_in = 16   # input feature dim
    F_hidden = 32
    F_out = 8
    L = 2   # num_layers

    # 生成邻接矩阵
    adj = generate_dummy_adj(N)

    # 初始化 model
    model = GraphTransformer(
        in_channels=F_in,
        hidden_channels=F_hidden,
        out_channels=F_out,
        adj=adj,
        num_layers=L,
        num_heads=2,   # 假设 transconv 支持 num_heads
        alpha=0.5,
        dropout=0.1,
        use_bn=True,
        use_residual=True,
        use_weight=True,
        graph_weight=0.7
    )

    # 生成 dummy 输入数据
    x = torch.randn(B, T, N, F_in)

    # forward 一下
    print("\n-- Testing GraphTransformer --")
    output = model(x)

    print("Input shape:", x.shape)
    print("Output shape:", output.shape)   # 期望 [B, T, F_out]

    # 试一下 attention 可不可以返回
    try:
        attentions = model.get_attentions(x)
        print("Attention shape:", attentions.shape if hasattr(attentions, 'shape') else "List of len {}".format(len(attentions)))
    except Exception as e:
        print("Attention extraction failed:", e)