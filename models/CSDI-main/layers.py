# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bLxA1ZktojW_RP2LUyVVSOPACucwzDms
"""

import scipy.sparse as sp

import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch_geometric.nn import GCNConv
import numpy as np


# def Attn_tem(heads=8, layers=1, channels=64):
#     encoder_layer = TransformerEncoderLayer_QKV(
#         d_model=channels, nhead=heads, dim_feedforward=64, activation="gelu"
#     )
#     return TransformerEncoder_QKV(encoder_layer, num_layers=layers)


def Conv1d_with_init(in_channels, out_channels, kernel_size):
    layer = nn.Conv1d(in_channels, out_channels, kernel_size)
    nn.init.kaiming_normal_(layer.weight)
    return layer


# def _get_activation_fn(activation):
#     if activation == "relu":
#         return F.relu
#     elif activation == "gelu":
#         return F.gelu
#     raise RuntimeError("activation should be relu/gelu, not {}".format(activation))


# def _get_clones(module, N):
#     return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


# Traditional Attention Mechanism.
# class TransformerEncoderLayer_QKV(nn.Module):
#     def __init__(self, d_model, nhead, dim_feedforward=1024, dropout=0.1, activation="relu"):
#         super().__init__()
#         self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
#         self.linear1 = nn.Linear(d_model, dim_feedforward)
#         self.dropout = nn.Dropout(dropout)
#         self.linear2 = nn.Linear(dim_feedforward, d_model)

#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.dropout1 = nn.Dropout(dropout)
#         self.dropout2 = nn.Dropout(dropout)

#         self.activation = _get_activation_fn(activation)
#     def __setstate__(self, state):
#         if 'activation' not in state:
#             state['activation'] = F.relu
#         super(TransformerEncoderLayer_QKV, self).__setstate__(state)

# when training process, 
# if for the src_mask: get by the target domain.

#     def forward(self, query, key, value, src_mask=None, src_key_padding_mask=None):
#         # [B, S, E] input
#         attn_output, _ = self.self_attn(query, key, value, attn_mask=src_mask,
#                                         key_padding_mask=src_key_padding_mask)
#         src = query + self.dropout1(attn_output)
#         src = self.norm1(src)

#         ff_output = self.linear2(self.dropout(self.activation(self.linear1(src))))
#         src = src + self.dropout2(ff_output)
#         src = self.norm2(src)
#         return src

# class TransformerEncoder_QKV(nn.Module):
#     __constants__ = ['norm']

#     def __init__(self, encoder_layer, num_layers, norm=None):
#         super(TransformerEncoder_QKV, self).__init__()
#         self.layers = _get_clones(encoder_layer, num_layers)
#         self.num_layers = num_layers
#         self.norm = norm

#     def forward(self, query, key, src, mask=None, src_key_padding_mask=None):
#     # src_key_padding_mask means whether mask or not mask.

#         output = src
#         for mod in self.layers:
#             output = mod(query, key, output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
#         if self.norm is not None:
#             output = self.norm(output)
#         return output

# # temporal attention:
# # itp_y: the temporal information from the source domain
# # as they share the same layers
# class TemporalLearning(nn.Module):
#     def __init__(self, channels, nheads, is_cross=False):
#         super().__init__()
#         self.is_cross = is_cross
#         self.time_layer = Attn_tem(heads=nheads, layers=1, channels=channels)
#         self.cond_proj = Conv1d_with_init(2 * channels, channels, 1) 
#     def forward(self, y, base_shape, itp_y=None):
#         B, channel, K, L = base_shape
#         if L == 1:
#             return y
#         y = y.reshape(B, channel, K, L).permute(0, 2, 1, 3).reshape(B * K, channel, L)
#         v = y.permute(2, 0, 1) # [L, B*K, C]
#         if self.is_cross:
#             itp_y = itp_y.reshape(B, channel, K, L).permute(0, 2, 1, 3).reshape(B * K, channel, L)
#             q = itp_y.permute(2, 0, 1)
#             y = self.time_layer(q, q, v).permute(1, 2, 0)
#         else:
#             y = self.time_layer(v, v, v).permute(1, 2, 0)
#         y = y.reshape(B, K, channel, L).permute(0, 2, 1, 3).reshape(B, channel, K * L)
#         return y

# # ---- Config ---- #
# import torch
# B = 2           # batch size
# C = 64          # channels
# K = 5           # spatial points (e.g., sensors)
# L = 12          # temporal steps
# device = 'cuda' if torch.cuda.is_available() else 'cpu'

# # ---- Dummy data ---- #
# input_y = torch.randn(B, C, K * L).to(device)          # Flattened [B, C, K*L]
# itp_y = torch.randn(B, C, K * L).to(device)            # For cross-attention mode
# base_shape = (B, C, K, L)

# # ---- Self-Attention Mode ---- #
# print("ðŸ§ª Testing TemporalLearning (self-attention)...")
# temporal_self = TemporalLearning(channels=C, nheads=4, is_cross=False).to(device)
# output_self = temporal_self(input_y, base_shape)
# print("Self-attention output shape:", output_self.shape)

# # ---- Cross-Attention Mode ---- #
# print("\nðŸ§ª Testing TemporalLearning (cross-attention)...")
# temporal_cross = TemporalLearning(channels=C, nheads=4, is_cross=True).to(device)
# output_cross = temporal_cross(input_y, base_shape, itp_y=itp_y)
# print("Cross-attention output shape:", output_cross.shape)

# # ---- Consistency Check ---- #
# assert output_self.shape == (B, C, K * L)
# assert output_cross.shape == (B, C, K * L)
# print("\nâœ… Temporal attention layer passed the shape check.")

class GCN(nn.Module):
    def __init__(self, in_channels, hidden_channels=64, out_channels=64,
                 adj=None, num_layers=2, dropout=0.5, save_mem=True, use_bn=True):
        super(GCN, self).__init__()

        assert adj is not None, "You must provide a precomputed adjacency matrix (adj)."
        assert adj.shape[0] == adj.shape[1], f"Adjacency matrix must be square, got {adj.shape}"

        # Convert dense numpy adjacency matrix to edge_index
        row, col = np.nonzero(adj)
        self.edge_index = torch.tensor(np.vstack([row, col]), dtype=torch.long)

        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()

        self.convs.append(GCNConv(in_channels, hidden_channels, cached=not save_mem))
        self.bns.append(nn.BatchNorm1d(hidden_channels))

        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_channels, hidden_channels, cached=not save_mem))
            self.bns.append(nn.BatchNorm1d(hidden_channels))

        self.convs.append(GCNConv(hidden_channels, out_channels, cached=not save_mem))

        self.dropout = dropout
        self.activation = F.relu
        self.use_bn = use_bn

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()
        for bn in self.bns:
            bn.reset_parameters()

    def forward(self, x):
        B, C, T = x.shape
        x = x.permute(0, 2, 1).reshape(B * T, C)  # [B*T, C]
        edge_index = self.edge_index.to(x.device)

        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            if self.use_bn:
                x = self.bns[i](x)
            x = self.activation(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.convs[-1](x, edge_index)
        x = x.reshape(B, T, -1).permute(0, 2, 1)  # [B, C_out, T]
        return x

# shared attention from target -> source domain.
# y: 
class SpatialLearning(nn.Module):
    def __init__(self, channels, nheads, device, is_adp, adj, proj_t, is_cross):
        super().__init__()
        self.is_cross = is_cross
        self.feature_layer = SpaDependLearning(channels, nheads=nheads, device=device, is_adp=is_adp, adj=adj,
                                      proj_t=proj_t, is_cross=is_cross)

    def forward(self, y, base_shape, support, itp_y=None):
        B, channel, K, L = base_shape
        if K == 1:
            return y
        y = self.feature_layer(y, base_shape, support, itp_y)
        return y

class SpaDependLearning(nn.Module):
    def __init__(self, channels, nheads, device, is_adp, adj, proj_t, is_cross=False):
        super().__init__()
        self.is_cross = is_cross
        self.GCN = GCN(in_channels=channels, hidden_channels=64, out_channels=channels, adj=adj)
        self.attn = Attn_spa(dim=channels, k=proj_t, heads=nheads)
        self.cond_proj = Conv1d_with_init(2 * channels, channels, 1)
        self.norm1_local = nn.GroupNorm(4, channels)
        self.norm1_attn = nn.GroupNorm(4, channels)
        self.ff_linear1 = nn.Linear(channels, channels * 2)
        self.ff_linear2 = nn.Linear(channels * 2, channels)
        self.norm2 = nn.GroupNorm(4, channels)

    def forward(self, y, base_shape, support, itp_y=None):
        B, channel, K, L = base_shape
        y_in1 = y
        # local spatial modeling
        # the GCN processes node-wise dependencies per time step
        # y_local = self.GCN(y, base_shape, support) # [B, C, K*L]
        # y_local = self.GCN(y.reshape(B, channel, K * L))
        # Step 1: Per-timestep Graph Convolution
        y_local = []
        for t in range(L):
            # y[:, :, :, t] shape: [B, C, K]
            y_t = y[:, :, :, t]  # (B, C, K)
            y_t = y_t.permute(0, 2, 1)  # (B, K, C)
            y_t = y_t.reshape(-1, channel)  # (B*K, C)
            
            # Run GCN on each batch (assumes shared edge_index for all)
            y_t_out = self.GCN(y_t)  # (B*K, C_out)
            y_t_out = y_t_out.reshape(B, K, channel).permute(0, 2, 1)  # (B, C, K)

            y_local.append(y_t_out.unsqueeze(-1))  # (B, C, K, 1)

        y_local = torch.cat(y_local, dim=-1)  # (B, C, K, L)
        y_local = y_local + y_in1  # # residual connection
        y_local = self.norm1_local(y_local) # Group Norm
        y_attn = y.reshape(B, channel, K, L).permute(0, 3, 1, 2).reshape(B * L, channel, K)
        # if using the cross attention
        # itp_y means the sequence y after interpolation.
        if self.is_cross:
            itp_y_attn = itp_y.reshape(B, channel, K, L).permute(0, 3, 1, 2).reshape(B * L, channel, K)
            y_attn = self.attn(y_attn.permute(0, 2, 1), itp_y_attn.permute(0, 2, 1)).permute(0, 2, 1)
        # if not cross_attention, we use the self attention mechanism.
        else:
            y_attn = self.attn(y_attn.permute(0, 2, 1)).permute(0, 2, 1)
        y_attn = y_attn.reshape(B, L, channel, K).permute(0, 2, 3, 1).reshape(B, channel, K * L)

        y_attn = y_in1 + y_attn
        y_attn = self.norm1_attn(y_attn)

        y_in2 = y_local + y_attn
        y = F.relu(self.ff_linear1(y_in2.permute(0, 2, 1)))
        y = self.ff_linear2(y).permute(0, 2, 1)
        y = y + y_in2

        y = self.norm2(y)
        return y


# standard multi-head self-attention mechanism.
class Attn_spa(nn.Module):
    def __init__(self, dim, k = 32, heads=8, dim_head=None, dropout=0.1, use_pos_enc=False):
        super().__init__()
        assert dim % heads == 0, 'dim must be divisible by heads'
        self.k = k
        self.heads = heads
        self.dim_head = dim_head or (dim // heads)

        inner_dim = self.dim_head * heads

        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_k = nn.Linear(dim, inner_dim, bias=False)
        self.to_v = nn.Linear(dim, inner_dim, bias=False)
        self.to_out = nn.Linear(inner_dim, dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        x: [B, N, D] (features at spatial nodes)
        """
        B, N, D = x.shape
        H = self.heads
        d_h = self.dim_head

        q = self.to_q(x).reshape(B, N, H, d_h).transpose(1, 2)  # [B, H, N, d_h]
        k = self.to_k(x).reshape(B, N, H, d_h).transpose(1, 2)  # [B, H, N, d_h]
        v = self.to_v(x).reshape(B, N, H, d_h).transpose(1, 2)  # [B, H, N, d_h]

        # Scaled dot-product attention
        attn_scores = torch.einsum('bhid,bhjd->bhij', q, k) * (d_h ** -0.5)  # [B, H, N, N]
        attn = attn_scores.softmax(dim=-1)
        attn = self.dropout(attn)

        out = torch.einsum('bhij,bhjd->bhid', attn, v)  # [B, H, N, d_h]
        out = out.transpose(1, 2).reshape(B, N, -1)  # [B, N, D]

        return self.to_out(out)


class CrossDomainFusionBlock(nn.Module):
    def __init__(self, embed_dim, num_heads=4, dropout=0.1, use_gating=True):
        super(CrossDomainFusionBlock, self).__init__()
        self.use_gating = use_gating

        # Target attends to Source
        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)

        # self-attention inside target domain
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)

        # Gating to mix cross-attn + self-attn representations
        if use_gating:
            self.gate_linear = nn.Linear(embed_dim * 2, embed_dim)

        self.norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt_input, src_input, src_mask=None, tgt_mask=None):
        """
        tgt_input: (B, T, D)   # Target domain features (e.g., partially observed)
        src_input: (B, S, D)   # Source domain features (e.g., fully observed)
        """

        # Target attends to Source
        # Target contains some missing variables, so target query important information from source domain.
        # using the attention mechanism.
        cross_output, cross_attn_weights = self.cross_attn(tgt_input, src_input, src_input, key_padding_mask=src_mask)

        # Optional: self-attention inside target
        self_output, self_attn_weights = self.self_attn(tgt_input, tgt_input, tgt_input, key_padding_mask=tgt_mask)

        # Combine cross and self with gating
        if self.use_gating:
            concat = torch.cat([self_output, cross_output], dim=-1)
            gate = torch.sigmoid(self.gate_linear(concat))
            fused = gate * self_output + (1 - gate) * cross_output
        else:
            fused = self_output + cross_output

        # Final layer norm and dropout
        out = self.norm(tgt_input + self.dropout(fused))
        return out, cross_attn_weights, self_attn_weights


class MultiModalFusionTransformer(nn.Module):
    def __init__(self, channels, hidden_dim, num_heads, adj, num_layers=1, dropout=0.1):
        super().__init__()
        # "is_cross" means the cross attention within temporal block
        # learn from different temporal sequences
        self.temporal = TemporalLearning(channels, nheads=num_heads, is_cross=False)
        # learn from different
        self.spatial = SpatialLearning(
            channels=channels,
            nheads=num_heads,
            device='cuda',
            is_adp=False,
            adj=adj,
            proj_t=64,
            is_cross=False
        )

        self.temporal_proj = nn.Linear(channels, hidden_dim)
        self.spatial_proj = nn.Linear(channels, hidden_dim)
        self.cross_proj = nn.Linear(channels, hidden_dim)

        self.cross_fusion = CrossDomainFusionBlock(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim * 3,
            nhead=num_heads,
            dropout=dropout,
            dim_feedforward=hidden_dim * 4,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.output_layer = nn.Linear(hidden_dim * 3, channels)
        self.adj = adj # Store adj

# no coords_src and coords_tgt in the 
    def forward(self, x_src, x_tgt, base_shape):
        y_temporal_src = self.temporal(x_src, base_shape, itp_y=None)
        # Pass coords_src to spatial
        y_spatial_src = self.spatial(x_src, base_shape, itp_y = x_src)

        y_temporal_tgt = self.temporal(x_tgt, base_shape, itp_y=x_src)
        # Pass coords_tgt to spatial
        y_spatial_tgt = self.spatial(x_tgt, base_shape, itp_y=x_src)

        # Cross fusion block: target from src
        B, C, T = y_temporal_tgt.shape
        tgt_embed = (y_temporal_tgt + y_spatial_tgt).permute(0, 2, 1)  # (B, T, C)
        src_embed = (y_temporal_src + y_spatial_src).permute(0, 2, 1)  # (B, T, C)

        fused_cross, cross_attn_weights, self_attn_weights = self.cross_fusion(tgt_embed, src_embed)  # (B, T, C)

        y_tem = self.temporal_proj(y_temporal_tgt.permute(0, 2, 1))
        y_spa = self.spatial_proj(y_spatial_tgt.permute(0, 2, 1))
        y_cro = self.cross_proj(fused_cross)

        fused = torch.cat([y_tem, y_spa, y_cro], dim=-1)
        fused = self.transformer(fused)
        out = self.output_layer(fused)
        out = out.permute(0, 2, 1)
        return out, cross_attn_weights, self_attn_weights


class CrossDomainFusionBlock(nn.Module):
    def __init__(self, embed_dim, num_heads=4, dropout=0.1, use_gating=True):
        super(CrossDomainFusionBlock, self).__init__()
        self.use_gating = use_gating

        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)

        if use_gating:
            self.gate_linear = nn.Linear(embed_dim * 2, embed_dim)

        self.norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt_input, src_input, src_mask=None, tgt_mask=None):
        cross_output, cross_attn_weights = self.cross_attn(tgt_input, src_input, src_input, key_padding_mask=src_mask)
        self_output, self_attn_weights = self.self_attn(tgt_input, tgt_input, tgt_input, key_padding_mask=tgt_mask)

        if self.use_gating:
            concat = torch.cat([self_output, cross_output], dim=-1)
            gate = torch.sigmoid(self.gate_linear(concat))
            fused = gate * self_output + (1 - gate) * cross_output
        else:
            fused = self_output + cross_output

        out = self.norm(tgt_input + self.dropout(fused))
        return out, cross_attn_weights, self_attn_weights

# ---------- Test Script ----------

if __name__ == "__main__":
    from generate_adj import get_similarity_pems04
    adj = get_similarity_pems04()

    B, C, K, L = 2, 64, 170, 1  # K matches adj.shape[0], L=1
    base_shape = (B, C, K, L)
    T = K * L  # T = 170

    x_src = torch.randn(B, C, T)  # [2, 64, 170]
    x_tgt = torch.randn(B, C, T)# [2, 64, 170]

    support_src = torch.eye(K)  # Identity for simplicity; can be replaced with real graph support
    support_tgt = torch.eye(K)

    # Add dummy spatial coordinates for testing
    # Assuming 2D coordinates for each of the K spatial points
    coords_src = torch.randn(K, 2) # [170, 2]
    coords_tgt = torch.randn(K, 2) # [170, 2]


    model = MultiModalFusionTransformer(
        channels=C,
        adj=adj,              # your precomputed (170, 170) matrix
        hidden_dim=64,
        num_heads=4,
        num_layers=2
    )

    out, cross_attn_weights, self_attn_weights = model(x_src, x_tgt, base_shape, support_src, support_tgt, coords_src, coords_tgt)
    print("Output shape:", out.shape)
    print("Cross-attention weights shape:", cross_attn_weights.shape)
    print("Self-attention weights shape:", self_attn_weights.shape)